import org.apache.spark._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.rdd.RDD

import org.apache.spark.sql.{Row,SparkSession,DataFrame}
import org.apache.spark.sql.functions.{col, from_json, unbase64, concat, lit}
import org.apache.spark.sql.types._
import org.apache.spark.streaming._
import org.apache.spark.{SparkConf, SparkContext}


import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import org.apache.logging.log4j.Level
import org.apache.log4j.PropertyConfigurator
import org.apache.logging.log4j.core.config.Configurator
//import org.apache.spark.internal.Logging

import java.net.Socket
import java.nio.charset.StandardCharsets
import java.io._
import java.util.Base64

import scala.sys.process._
import scala.language.postfixOps
import scala.io.Source

import org.json4s._
import org.json4s.JValue
import org.json4s.jackson.JsonMethods._
import org.json4s.JsonAST.JObject

//import org.apache.commons.codec.binary.Base64

object licensectrl {

  // def readJsonFileToDataFrame(filePath: String, spark: SparkSession): DataFrame = {
  //   val schema = new StructType()
  //   .add("license_key", StringType)
  //   .add("category", StringType)

  //   val jsonStringsRDD = Source.fromFile(filePath).getLines().mkString("\n")
  //   val json = parse(jsonStringsRDD).asInstanceOf[JArray].arr.asInstanceOf[Seq[JObject]]

  //   val rowsRDD = spark.sparkContext.parallelize(json).map  { item =>
  
  //     val license_key = (item \ "license_key").asInstanceOf[JString].s
  //     val category = (item \ "category").asInstanceOf[JString].s
  //     Row(license_key, category)
  //   }

  //   val df = spark.createDataFrame(rowsRDD, schema)
  //   df.createOrReplaceTempView("licenses")

  //   df
  // }  

  def readJsonFileToMap(filePath: String, spark: SparkSession): Map[String, String] = {
    import scala.collection.mutable.Map
    val jsonStringsRDD = Source.fromFile(filePath).getLines().mkString("\n")
    val json = parse(jsonStringsRDD).asInstanceOf[JArray].arr.asInstanceOf[Seq[JObject]]

    val resultMap = json.map  { item =>
      val license_key = (item \ "license_key").asInstanceOf[JString].s
      val category = (item \ "category").asInstanceOf[JString].s
      (license_key, category)
    }.toMap

    resultMap
  }    


  var logger = LogManager.getLogger(getClass().getName());

  var cumulativeRDD: RDD[(String, Int)] = _

  def main(args: Array[String]): Unit = {

    //Logger.getLogger("org").setLevel(Level.OFF)
    //Logger.getLogger("akka").setLevel(Level.OFF)
    //setStreamingLogLevels()
    Configurator.setRootLevel(Level.FATAL)
    

    val conf = new SparkConf().setAppName("licensectrl")

    // Print all the configuration settings
    val configSettings = conf.getAll
    println("SparkConf Settings:")
    configSettings.foreach { case (key, value) =>
      println(s"$key: $value")
    }

    val ssc = new StreamingContext(conf, Seconds( Integer.parseInt(args(3)) ))
    //ssc.checkpoint("checkpoint-folder")
    //ssc.sparkContext.setLogLevel("FATAL")


    // Get the SparkContext's statusTracker
    val statusTracker = ssc.sparkContext.statusTracker
    // Get application id 
    val id = ssc.sparkContext.applicationId

    println(s"spark application id: application_$id")

    // Get the list of all the executor IDs
    //val executorIds = statusTracker.getExecutorIds()

    // Print executor information
    /*executorIds.foreach { executorId =>
      val executorInfo = statusTracker.getExecutorInfo(executorId)
      println(s"Executor ID: $executorId")
      println(s"Host: ${executorInfo.executorHost}")
      println(s"State: ${executorInfo.isActive}")
      // You can access more executor information as needed
    }*/



    // Create a DStream that will connect to hostname:port, like localhost:9999
    //val lines = ssc.socketTextStream("131.114.3.199", 9999)
    println("Connect to " + args(1) + ":"+args(2)+ " " + Integer.parseInt(args(2))  )
    //val lines = ssc.socketTextStream(args(1), Integer.parseInt(args(2)))

    val schema = new StructType()
      .add("project_id", StringType, nullable = true)
      .add("file_basename", StringType, nullable = true)
      .add("data", StringType, nullable = true)   
 
    val schemaMap = StructType(
      Array (
      StructField("project_id", StringType,  nullable = true),
      StructField("file_basename", StringType, nullable = true),
      StructField("license", StringType, nullable = true),
      StructField("category", StringType, nullable = true)
    )
    )
          

    // val rawStreams = (1 to Integer.parseInt(args(0))).map(_ =>
    //                   ssc.rawSocketStream[String](args(1), Integer.parseInt(args(2)), StorageLevel.MEMORY_ONLY_SER_2)).toArray
    val rawStreams = (1 to args(0).toInt).map(_ =>
                      ssc.socketTextStream(args(1), Integer.parseInt(args(2)), StorageLevel.MEMORY_ONLY))

    val spark = SparkSession.builder().getOrCreate()


    val filePath = "/home/ubuntu/Software-Heritage-Analytics/Orchestrator/app/licensectrl/src/main/scala/scancode_index.json"
    val licenseMap:Map[String, String] = readJsonFileToMap(filePath, spark)
    
    licenseMap.foreach { case (key, value) =>
      println(s"Key: $key, Value: $value")
    }


    val union = ssc.union(rawStreams)

    val EOSCounter = spark.sparkContext.longAccumulator("EOSCounter")
    var dataList:List[Row] = List()

    // TODO: Path salvataggio file (Da settare al lancio)
    val file_path = "/tmp/testramdisk" 

    union.foreachRDD { rdd =>
      if (!rdd.isEmpty()) {
        
      val rowRDD = rdd.map { item =>
        // implicit val formats = DefaultFormats
        val json = parse(item).asInstanceOf[JObject]
        val project_id =    compact(render((json \ "project_id"))).replace("\"", "")
        val file_basename = compact(render((json \ "file_basename"))).replace("\"", "")
        // val file_name =     compact(render((json \ "file_name"))).replace("\"", "")
        // val file_type =     compact(render((json \ "file_type"))).replace("\"", "")
        val data =          compact(render((json \ "data"))).replace("\"", "")
        // Row(project_id, file_basename, file_name, file_type, data)
        // creo 2 colonne a null per licenza e gruppo
        Row(project_id, file_basename, data)
      }

      // Creazione dataframe
      val rddDataFrame = spark.createDataFrame(rowRDD, schema)
      rddDataFrame.show()
      
      val rddData = rddDataFrame.rdd.map{ row =>
        import org.apache.spark.sql.functions._
        import spark.implicits._
      // Estraggo i dati dalla riga
        val project_id = row.getString(0)
        val file_basename = row.getString(1)
        val data_64 = row.getString(2)

        val file_name = project_id+"_"+file_basename

        // Decode del base 64
        val data_bytes = Base64.getDecoder.decode(data_64)
        val data = new String(data_bytes, "UTF-8") 

        // Nome file salvato
        val new_file = s"$file_path/$file_name"

        // Scrivo file su disco per scancode
        val outputFile = new File(new_file)
        val writer = new BufferedWriter(new FileWriter(outputFile, true))
        // Write item to the file6
        writer.write(data)
        writer.close()

        // ESTRAZIONE DELLA LICENZA 
        
        // Lancio scancode su file  
        val json_file = s"$file_path/$file_name.json"
        val process = Runtime.getRuntime.exec(s"/home/ubuntu/scancode-toolkit/scancode -clpeui -q -n 1 --json-pp ${json_file} ${new_file}")
        process.waitFor()  // Aspetta che il processo esterno finisca        

        val file_content = Source.fromFile(json_file).getLines().mkString("\n")
        val file_content_json = parse(file_content).asInstanceOf[JObject]

        // Estrai il campo "license_detections" come lista di oggetti JValue
        val license_detections = (file_content_json \ "license_detections").asInstanceOf[JArray]

        // Estrai il campo "license_expression" da ciascun oggetto "license_detections"
        val expressions = license_detections.arr.map { obj =>
            (obj \ "license_expression").asInstanceOf[JString].s
        }
        
        var expression = ""
        var category = ""

        if (expressions.nonEmpty){
          expression = expressions(0)
          val found: Option[String] = licenseMap.get(expression)

          found match {
            case Some(value) => category = value
            case None        => println(s"Key '$expression' not found in the map")
          }
        }         

        Row(project_id, file_basename, expression, category)
      }

      // se category "" => Unstated License
      val nuovoDF = spark.createDataFrame(rddData, schemaMap).toDF("project_id", "file_basename", "expression", "category")

      nuovoDF.show()

      // val rddComplete = nuovoDF.rdd.map { row =>
      //   val spark = SparkSession.builder.config(rdd.sparkContext.getConf).getOrCreate()
      //   import org.apache.spark.sql.functions._
      //   import spark.implicits._
      //   val project_id = row.getString(0)
      //   val file_basename = row.getString(1)
      //   val expression = row.getString(2)

      //   val category = licenseDF(expression)
        // val filtered = licenseDF.filter($"license_key" === expression)
        // if (!filtered.isEmpty) {
        //   category = filtered.select("category").first().getString(0)
        // }         
      //   Row(project_id, file_basename, expression)
      // }

      // val completeDF = spark.createDataFrame(rddComplete, schemaCompleteDF).toDF("project_id", "file_basename", "expression", "category")
      // completeDF.show()



      // val rddData = rddDataFrame.drop("data")
      // rddData.show()
      // rddDataFrame.unpersist()

      // FOREACH del datarfame e rifare le cose fatte sul file alla fine aggiungere una nuova colonna al df e droppiamo la colonna data
      }
      else {
        println("empty")
      }


    //union.foreachRDD { partition =>
      //val spark = SparkSession.builder.config(rdd.sparkContext.getConf).getOrCreate()

      //val EOSCounter = spark.sparkContext.longAccumulator("EOSCounter")

//       println("union.foreachRDD")
//       if (!rdd.isEmpty()) {

//         import spark.implicits._
               
//         println("not empty")
//         //rdd.foreachPartition { partition =>
//           println("partition")

//           // Inizializza una lista per accumulare i dati
//           dataList = List.empty[Row] 

//           rdd.foreach { item =>

//           // Ottieni l'ID del worker
//           val workerId = ssc.sparkContext.getConf.get("spark.worker.id")

//             logger.fatal(s"[$workerId] start list: $dataList")
//           //partition.foreach { item =>
//             val jitem = parse(item).asInstanceOf[JObject]
//             val name = jitem \ "file_basename"
//             val project_id = jitem \ "project_id"
//             val data_64 = compact(render(jitem \ "data")).replace("\"","") //base64
//             val data_bytes = Base64.getDecoder.decode(data_64)
//             val data = new String(data_bytes, "UTF-8")
//             val file_path = "/tmp/testramdisk" 
//             val projectIdString = compact(render(project_id)).replace("\"","")
//             val nameString = compact(render(name)).replace("\"","")

//             val file_name = projectIdString+"_"+nameString
            // val new_file = s"$file_path/$file_name"
    

            // val outputFile = new File(new_file)
            // val writer = new BufferedWriter(new FileWriter(outputFile, true))
            // // Write item to the file6
            // writer.write(data)
            // writer.close()
          
//            //}

//             val json_file = s"$file_path/$file_name.json"
//             val process = Runtime.getRuntime.exec(s"/home/ubuntu/scancode-toolkit/scancode -clpeui -q -n 1 --json-pp ${json_file} ${new_file}")
//             process.waitFor()  // Aspetta che il processo esterno finisca

             
//             // Eventualmente, leggi l'output del processo o gestisci gli errori
//             // val output = scala.io.Source.fromInputStream(process.getInputStream).mkString
//             // println(output)
//             val fileContent = Source.fromFile(json_file).getLines().mkString("\n")
//             val fileContentJson = parse(fileContent).asInstanceOf[JObject]

//             // Estrai il campo "license_detections" come lista di oggetti JValue
//             val licenseDetections = (fileContentJson \ "license_detections").asInstanceOf[JArray]

//             // Estrai il campo "license_expression" da ciascun oggetto "license_detections"
//             val expressions = licenseDetections.arr.map { obj =>
//                 (obj \ "license_expression").asInstanceOf[JString].s
//             }
            
//             if (expressions.nonEmpty){
//               //dataList = Row(projectIdString,nameString,expressions(0),"Cat") :: dataList
//               val row = Row(projectIdString,nameString,expressions(0),"Cat")
//               // dataList = dataList :+ row
//               dataList = row :: dataList

//               // logger.fatal(expressions(0))
//               // logger.fatal(row)
//               logger.fatal(s"New expression: $row")
//               logger.fatal(s"After put: $dataList")
//               logger.fatal(s"Counter: ${dataList.size}")
// //            spark.read.json(fileContent)
//             } else {
//               logger.fatal("Expression Empty")
//             }

//               //          val cmd = s"/home/ubuntu/scancode-toolkit/scancode -clpeui -q -n 1 --json-pp $json_file $file"
//               //          val output = cmd!!
                    
//                       //println("output:" + output)

//                       //val filePathIn = s"/tmp/testramdisk/${filename}.json"
//                       /*try {
//                         val fileContent = Source.fromFile(filePathIn).getLines().mkString("\n")
//                         println(s"Contenuto del file:\n$fileContent")
//                       } catch {
//                         case e: Exception =>
//                           println(s"Errore durante la lettura del file: ${e.getMessage}")
//                       }*/
                      


//                       //val result = decoded_data.filter(col("decoded_data").contains("Lorem"))
//                       //result.show()
                    
//                       /*
//                       if (cumulativeRDD == null){
//                         cumulativeRDD = result.rdd
//                       }else{
//                         val ret = cumulativeRDD.fullOuterJoin(result.rdd)
//                         cumulativeRDD = ret.map[(String, Int)]({
//                           vv: (String, (Option[Int], Option[Int])) =>  {(vv._1, vv._2._1.getOrElse(0) + vv._2._2.getOrElse(0))}
//                         })
//                       }
//                       cumulativeRDD.foreach(println)
//                       */

//                       /*
//                       val words = decoded_data
//                         .flatMap { row => row.toString().substring(1, row.toString().length-1).split("\n")}
//                         .flatMap { row => row.toString().split(" ")}

//                       val pairs = words.map(word => (word, 1))
                      
//                       val wordCounts = pairs.groupBy(col("_1")).count()

//                       wordCounts.show()
//                       */

//         }
//           logger.fatal(s"Out foreach loop: ${dataList}")
//           // val newRdd = ssc.sparkContext.getOrCreate().makeRDD(dataList)

//           // val df = spark.createDataFrame(newRdd, schema=schemaDF)

//           // df.show()
//             //val fileDF = spark.read.json(latest_json)
//             //fileDF.printSchema()
//         //}

//         // Test end of all streams     
//         if ( EOSCounter.value == args(0).toInt ) {

//         //if ( dfJSON.filter(col("project_id").contains("EOS") ) {i
//             println("EOS - App exit")
//             //Thread.sleep(Integer.parseInt(args(2))*2)
//             //irdd.context.stop()
//             ssc.stop(true)
//             System.exit(0)


//         }

        
        
//       } else {
//         println("RDD empty")
//       }
      
    }


    //println("wordcount END Computation ")
    ssc.start() // Start the computation
    ssc.awaitTermination() // Wait for the computation to terminate
    
  
  }


  def sendResults(rdd: RDD[(String, Int)]): Unit = {
    if (!rdd.isEmpty()) {
      // Connessione al socket
      val sock = new Socket("localhost", 9998)
      // Invia i risultati
      sock.getOutputStream.write(rdd.collect().toString.getBytes(StandardCharsets.UTF_8))
      // Chiudi la connessione
      sock.close()
    }
  }

}
