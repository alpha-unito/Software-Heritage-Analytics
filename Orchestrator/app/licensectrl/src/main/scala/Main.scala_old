import org.apache.spark._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.{col, from_json, unbase64, concat, lit}
import org.apache.spark.sql.types.{IntegerType, StringType, StructType}
import org.apache.spark.streaming._
import org.apache.spark.{SparkConf, SparkContext}


import org.apache.logging.log4j.LogManager;
import org.apache.logging.log4j.Logger;

import org.apache.logging.log4j.Level
import org.apache.log4j.PropertyConfigurator
import org.apache.logging.log4j.core.config.Configurator
import org.apache.spark.internal.Logging

import java.net.Socket
import java.nio.charset.StandardCharsets
import java.io._

import scala.sys.process._
import scala.language.postfixOps
import scala.io.Source

import scala.util.parsing.json.JSONObject
import org.json4s.JsonAST.JObject
import org.apache.commons.codec.binary.Base64


object licensectrl {

  var logger = LogManager.getLogger(getClass().getName());

  var cumulativeRDD: RDD[(String, Int)] = _

  def main(args: Array[String]): Unit = {

    //Logger.getLogger("org").setLevel(Level.OFF)
    //Logger.getLogger("akka").setLevel(Level.OFF)
    //setStreamingLogLevels()
    Configurator.setRootLevel(Level.WARN)
    

    val conf = new SparkConf().setAppName("licensectrl")

    // Print all the configuration settings
    val configSettings = conf.getAll
    println("SparkConf Settings:")
    configSettings.foreach { case (key, value) =>
      println(s"$key: $value")
    }

    val ssc = new StreamingContext(conf, Seconds( Integer.parseInt(args(3)) ))
    //ssc.checkpoint("checkpoint-folder")
    //ssc.sparkContext.setLogLevel("FATAL")


    // Get the SparkContext's statusTracker
    val statusTracker = ssc.sparkContext.statusTracker

    // Get the list of all the executor IDs
    //val executorIds = statusTracker.getExecutorIds()

    // Print executor information
    /*executorIds.foreach { executorId =>
      val executorInfo = statusTracker.getExecutorInfo(executorId)
      println(s"Executor ID: $executorId")
      println(s"Host: ${executorInfo.executorHost}")
      println(s"State: ${executorInfo.isActive}")
      // You can access more executor information as needed
    }*/



    // Create a DStream that will connect to hostname:port, like localhost:9999
    //val lines = ssc.socketTextStream("131.114.3.199", 9999)
    println("Connect to " + args(1) + ":"+args(2)+ " " + Integer.parseInt(args(2))  )
    //val lines = ssc.socketTextStream(args(1), Integer.parseInt(args(2)))

    val schema = new StructType()
      .add("project_id", StringType, nullable = true)
      .add("file_basename", StringType, nullable = true)
      .add("file_name", StringType, nullable = true)
      .add("file_type", StringType, nullable = true)
      .add("data", StringType, nullable = true)


    // val rawStreams = (1 to Integer.parseInt(args(0))).map(_ =>
    //                   ssc.rawSocketStream[String](args(1), Integer.parseInt(args(2)), StorageLevel.MEMORY_ONLY_SER_2)).toArray
    val rawStreams = (1 to args(0).toInt).map(_ =>
                      ssc.socketTextStream(args(1), Integer.parseInt(args(2)), StorageLevel.MEMORY_AND_DISK))
    val spark = SparkSession.builder.getOrCreate()

    val union = ssc.union(rawStreams)

    val EOSCounter = spark.sparkContext.longAccumulator("EOSCounter")

    union.foreachRDD { rdd =>
    //union.foreachRDD { partition =>
      //val spark = SparkSession.builder.config(rdd.sparkContext.getConf).getOrCreate()

      //val EOSCounter = spark.sparkContext.longAccumulator("EOSCounter")


      logger.info("tu ma maiala");
      
      if (!rdd.isEmpty()) {

        import spark.implicits._
        

        rdd.foreachPartition { partition =>


          partition.foreach { item =>
        
        // Convert RDD[String] to DataFrame
        //val wordsDataFrame = rdd.toDF("raw_data")
        /*val wordsDataFrame = spark.read.json(item)


        val dfJSON = wordsDataFrame.withColumn("json", from_json(col("raw_data"), schema)).select("json.*")


        EOSCounter.add(dfJSON.filter(col("project_id").contains("EOS")).count())
        val dfJSON_cleaned = dfJSON.na.drop()


        val dfJSON_decoded = dfJSON_cleaned.withColumn("decoded_data", unbase64(col("data")).cast("string"))

        val decoded_data = dfJSON_decoded.select(col("project_id"),col("file_basename"),col("decoded_data"))
        //dfJSON_decoded.show()
       
        
        //val concatenatedDF = dfJSON.withColumn("concatenated_field", concat(col("project_id"), lit("_"), col("file_basename"), lit("_"), col("file_name")))
        val concatenatedDF = dfJSON.withColumn("concatenated_field", concat(col("project_id"), lit("_"), col("file_basename")))
        val filename = s"${concatenatedDF.select("concatenated_field").first().getString(0)}"
        */
        println(s"--------->${filename}------") 
        val filePath = s"/tmp/testramdisk/${filename}"
        /*val writer = new PrintWriter(filePath)
        println(filePath)
        try {
          writer.println(dfJSON_decoded)
        } finally {
          writer.close()
        }*/



        
       /* rdd.foreachPartition { partition =>
          // Within this function, you can write files for each worker
          // Each worker processes its own partition of the RDD
          val outputFile = new File("/tmp/worker_output2.txt")
          val writer = new BufferedWriter(new FileWriter(outputFile, true))

          partition.foreach { item =>
            // Write item to the file
            writer.write(item + "\n")
          }

          writer.close()
        }*/
        

        val cmd = s"/home/ubuntu/scancode-toolkit/scancode -clpeui -q -n 1 --json-pp  /tmp/testramdisk/${filename}.json /tmp/testramdisk/${filename}"
        //val output = cmd!!
      
        //println("output:" + output)

        val filePathIn = s"/tmp/testramdisk/${filename}.json"
        /*try {
          val fileContent = Source.fromFile(filePathIn).getLines().mkString("\n")
          println(s"Contenuto del file:\n$fileContent")
        } catch {
           case e: Exception =>
            println(s"Errore durante la lettura del file: ${e.getMessage}")
        }*/
        


        val result = decoded_data.filter(col("decoded_data").contains("Lorem"))
        //result.show()
       
        /*
        if (cumulativeRDD == null){
          cumulativeRDD = result.rdd
        }else{
          val ret = cumulativeRDD.fullOuterJoin(result.rdd)
          cumulativeRDD = ret.map[(String, Int)]({
            vv: (String, (Option[Int], Option[Int])) =>  {(vv._1, vv._2._1.getOrElse(0) + vv._2._2.getOrElse(0))}
          })
        }
        cumulativeRDD.foreach(println)
        */

        /*
        val words = decoded_data
          .flatMap { row => row.toString().substring(1, row.toString().length-1).split("\n")}
          .flatMap { row => row.toString().split(" ")}

        val pairs = words.map(word => (word, 1))
        
        val wordCounts = pairs.groupBy(col("_1")).count()

        wordCounts.show()
        */

          }
        }

        // Test end of all streams     
        if ( EOSCounter.value == args(0).toInt ) {

        //if ( dfJSON.filter(col("project_id").contains("EOS") ) {i
            println("EOS - App exit")
            //Thread.sleep(Integer.parseInt(args(2))*2)
            //irdd.context.stop()
            ssc.stop(true)
            System.exit(0)


        }

        
        
      } else {
        println("RDD empty")
      }
      
    }


    //println("wordcount END Computation ")
    ssc.start() // Start the computation
    ssc.awaitTermination() // Wait for the computation to terminate
    
  
  }


  def sendResults(rdd: RDD[(String, Int)]): Unit = {
    if (!rdd.isEmpty()) {
      // Connessione al socket
      val sock = new Socket("localhost", 9998)
      // Invia i risultati
      sock.getOutputStream.write(rdd.collect().toString.getBytes(StandardCharsets.UTF_8))
      // Chiudi la connessione
      sock.close()
    }
  }

}
